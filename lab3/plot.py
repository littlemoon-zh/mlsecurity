import numpy as np
import matplotlib.pyplot as plt
from lab3.eval import main

if __name__ == '__main__':
    frac = []
    accs = []
    asrs = []
    for i in range(1, 61):
        modelname = f'models/prune{i}.h5'
        clean = './data/cl/test.h5'
        bad = './data/bd/bd_test.h5'

        acc, asr = main(modelname, clean, bad)

        frac.append(i / 60)
        accs.append(acc)
        asrs.append(asr)
        print(i, acc, asr)
        print(i)

    plt.plot(frac, accs, label='acc')
    plt.plot(frac, asrs, label='asr')
    plt.legend()

    plt.savefig('res.png')
    plt.show()


'''
/opt/anaconda3/envs/ml/bin/python /Users/yhx/code/git_projects/mlsecurity/lab3/plot.py
2021-12-16 15:25:13.780178: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
1 98.62042088854248 100.0
1
2 98.62042088854248 100.0
2
3 98.62042088854248 100.0
3
4 98.62042088854248 100.0
4
5 98.62042088854248 100.0
5
6 98.62042088854248 100.0
6
7 98.62042088854248 100.0
7
8 98.62042088854248 100.0
8
9 98.62042088854248 100.0
9
10 98.62042088854248 100.0
10
11 98.62042088854248 100.0
11
12 98.62042088854248 100.0
12
13 98.62042088854248 100.0
13
14 98.62042088854248 100.0
14
15 98.62042088854248 100.0
15
16 98.62042088854248 100.0
16
17 98.62042088854248 100.0
17
18 98.62042088854248 100.0
18
19 98.62042088854248 100.0
19
20 98.62042088854248 100.0
20
21 98.62042088854248 100.0
21
22 98.62042088854248 100.0
22
23 98.62042088854248 100.0
23
24 98.62042088854248 100.0
24
25 98.62042088854248 100.0
25
26 98.62042088854248 100.0
26
27 98.62042088854248 100.0
27
28 98.62042088854248 100.0
28
29 98.62042088854248 100.0
29
30 98.62042088854248 100.0
30
31 98.62042088854248 100.0
31
32 98.62042088854248 100.0
32
33 98.62042088854248 100.0
33
34 98.61262665627436 100.0
34
35 98.61262665627436 100.0
35
36 98.60483242400623 100.0
36
37 98.60483242400623 100.0
37
38 98.58924395947 100.0
38
39 98.55027279812938 100.0
39
40 98.53468433359313 100.0
40
41 98.28526890101324 100.0
41
42 98.269680436477 100.0
42
43 97.88776305533905 100.0
43
44 97.66173031956352 100.0
44
45 95.90023382696803 100.0
45
46 95.5261106780982 99.97661730319564
46
47 95.22213561964146 99.97661730319564
47
48 94.77007014809041 99.98441153546376
48
49 94.17770849571318 99.97661730319564
49
50 92.50974279033515 80.576773187841
50
51 89.84411535463757 80.6469212782541
51
52 84.54403741231489 77.20966484801247
52
53 76.30553390491036 36.26656274356976
53
54 44.67653936087295 16.03273577552611
54
55 27.918939984411534 3.72564302416212
55
56 13.834762275915821 16.399064692127826
56
57 5.728760717069369 0.303975058456742
57
58 1.5198752922837102 0.0
58
59 0.646921278254092 0.0
59
60 0.0779423226812159 0.0
60

'''